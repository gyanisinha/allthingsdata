Here is what you can do for processing large xml:
create new environment and upload custom library. Maven Repository: com.databricks » spark-xml_2.12 » 0.18.0. Save and publish.
read using pyspark:
 
 
df = spark.read.format("com.databricks.spark.xml").option("rowTag", "dataset").(load("Files/xml/example.xml")
df.printSchema()
# Show the DataFrame (optional)
df.show()
# Write the DataFrame to Parquet format or any other format as needed
df.write.parquet("Files/processed/example.parquet")
